{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change Tweet Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are tasked to create a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainingdata = pd.read_csv(\"train.csv\").fillna(' ')\n",
    "Testdata = pd.read_csv(\"test_with_no_labels.csv\").fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainingdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (15819, 3)\n",
      "Columns: Index(['sentiment', 'message', 'tweetid'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = Trainingdata.copy()\n",
    "test_df = Testdata.copy()\n",
    "\n",
    "print('Dataset size:',train_df.shape)\n",
    "print('Columns:',train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'sentiment', data = train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15819, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = Trainingdata.drop_duplicates()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0        625221\n",
       "1        126103\n",
       "2        698562\n",
       "3        573736\n",
       "4        466954\n",
       "          ...  \n",
       "15814     22001\n",
       "15815     17856\n",
       "15816    384248\n",
       "15817    819732\n",
       "15818    806319\n",
       "Name: tweetid, Length: 15819, dtype: int64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tweetid.unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import string\n",
    "\n",
    "def remove_emoji(tweet_):\n",
    "    characters = [str for str in tweet_]\n",
    "    list_of_emoji = [i for i in characters if i in emoji.UNICODE_EMOJI]\n",
    "    clean_tweet = ' '.join([str for str in tweet_.split() if not any(j in str for j in list_of_emoji)])\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []\n",
    "\n",
    "for index, rows in train_df['message'].iteritems():\n",
    "    rows =  remove_emoji(rows)\n",
    "    tweets_list.append(rows)\n",
    "    \n",
    "train_df['message'] = tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Line breaks, URL's, Numbers, Capital letters, & Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def tweet_preprocessor(tweet):\n",
    "    tweet = tweet.replace('\\n', ' ')                           # remove line breaks\n",
    "    tweet = re.sub(r\"\\bhttps://t.co/\\w+\", '', tweet)           # remove URL's\n",
    "    tweet = re.sub('\\w*\\d\\w*', ' ', tweet)                     # remove numbers\n",
    "    tweet = re.sub('[%s]' % re.escape(string.punctuation), ' ',   \n",
    "            tweet.lower())                                     # remove capital letters and punctuation\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in train_df.iterrows():\n",
    "    train_df.at[i,'message'] = tweet_preprocessor(row['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn t think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it s not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  rawstory  researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker  wired     was a pivotal year in...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  soynoviodetodas  it s    and a racist  sex...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  ezlusztig  they took down the material on ...</td>\n",
       "      <td>22001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  washingtonpost  how climate change could b...</td>\n",
       "      <td>17856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>notiven  rt  nytimesworld  what does trump act...</td>\n",
       "      <td>384248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>rt     hey liberals the climate change crap is...</td>\n",
       "      <td>819732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>rt  chet cannon    kurteichenwald s  climate c...</td>\n",
       "      <td>806319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid\n",
       "0              1  polyscimajor epa chief doesn t think carbon di...   625221\n",
       "1              1  it s not like we lack evidence of anthropogeni...   126103\n",
       "2              2  rt  rawstory  researchers say we have three ye...   698562\n",
       "3              1   todayinmaker  wired     was a pivotal year in...   573736\n",
       "4              1  rt  soynoviodetodas  it s    and a racist  sex...   466954\n",
       "...          ...                                                ...      ...\n",
       "15814          1  rt  ezlusztig  they took down the material on ...    22001\n",
       "15815          2  rt  washingtonpost  how climate change could b...    17856\n",
       "15816          0  notiven  rt  nytimesworld  what does trump act...   384248\n",
       "15817         -1  rt     hey liberals the climate change crap is...   819732\n",
       "15818          0  rt  chet cannon    kurteichenwald s  climate c...   806319\n",
       "\n",
       "[15819 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization -> chops each tweet into a collection of individual words (i.e. tokens)\n",
    "#Lemmatization -> Aims to cut each word down to its base form \n",
    "                #(e.g. laugh, laughs, laughing, laughed would be reduced to laugh)\n",
    "                #This reduces the complexity of analysis by reducing number of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# NLP Libraries\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    Tokenized_Doc=[]\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    for tweet in df['message']:\n",
    "        \n",
    "        onlyWords = re.sub('[^a-zA-Z]', ' ', tweet)     #keeping only words\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')      #removing URLs\n",
    "        review_ = url.sub(r'',onlyWords)\n",
    "        html=re.compile(r'<.*?>')\n",
    "        review_ = html.sub(r'',review_)\n",
    "        emojis = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"     # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"     # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"     # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"     # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        review_ = emojis.sub(r'',review_)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(review_)\n",
    "        gen_tweets = [lemm.lemmatize(token) for token in tokens if not token in stopWords]\n",
    "        Tokenized_Doc.append(gen_tweets)\n",
    "        df['tweet tokens'] = pd.Series(Tokenized_Doc)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "train_df = data_preprocessing(train_df)\n",
    "test_df = data_preprocessing(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tweet tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesn t think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, think, carbon, diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it s not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[like, lack, evidence, anthropogenic, global, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  rawstory  researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[rt, rawstory, researcher, say, three, year, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker  wired     was a pivotal year in...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, pivotal, year, war, clim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  soynoviodetodas  it s    and a racist  sex...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[rt, soynoviodetodas, racist, sexist, climate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15814</th>\n",
       "      <td>1</td>\n",
       "      <td>rt  ezlusztig  they took down the material on ...</td>\n",
       "      <td>22001</td>\n",
       "      <td>[rt, ezlusztig, took, material, global, warmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>2</td>\n",
       "      <td>rt  washingtonpost  how climate change could b...</td>\n",
       "      <td>17856</td>\n",
       "      <td>[rt, washingtonpost, climate, change, could, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15816</th>\n",
       "      <td>0</td>\n",
       "      <td>notiven  rt  nytimesworld  what does trump act...</td>\n",
       "      <td>384248</td>\n",
       "      <td>[notiven, rt, nytimesworld, trump, actually, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15817</th>\n",
       "      <td>-1</td>\n",
       "      <td>rt     hey liberals the climate change crap is...</td>\n",
       "      <td>819732</td>\n",
       "      <td>[rt, hey, liberal, climate, change, crap, hoax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15818</th>\n",
       "      <td>0</td>\n",
       "      <td>rt  chet cannon    kurteichenwald s  climate c...</td>\n",
       "      <td>806319</td>\n",
       "      <td>[rt, chet, cannon, kurteichenwald, climate, ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15819 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            message  tweetid  \\\n",
       "0              1  polyscimajor epa chief doesn t think carbon di...   625221   \n",
       "1              1  it s not like we lack evidence of anthropogeni...   126103   \n",
       "2              2  rt  rawstory  researchers say we have three ye...   698562   \n",
       "3              1   todayinmaker  wired     was a pivotal year in...   573736   \n",
       "4              1  rt  soynoviodetodas  it s    and a racist  sex...   466954   \n",
       "...          ...                                                ...      ...   \n",
       "15814          1  rt  ezlusztig  they took down the material on ...    22001   \n",
       "15815          2  rt  washingtonpost  how climate change could b...    17856   \n",
       "15816          0  notiven  rt  nytimesworld  what does trump act...   384248   \n",
       "15817         -1  rt     hey liberals the climate change crap is...   819732   \n",
       "15818          0  rt  chet cannon    kurteichenwald s  climate c...   806319   \n",
       "\n",
       "                                            tweet tokens  \n",
       "0      [polyscimajor, epa, chief, think, carbon, diox...  \n",
       "1      [like, lack, evidence, anthropogenic, global, ...  \n",
       "2      [rt, rawstory, researcher, say, three, year, a...  \n",
       "3      [todayinmaker, wired, pivotal, year, war, clim...  \n",
       "4      [rt, soynoviodetodas, racist, sexist, climate,...  \n",
       "...                                                  ...  \n",
       "15814  [rt, ezlusztig, took, material, global, warmin...  \n",
       "15815  [rt, washingtonpost, climate, change, could, b...  \n",
       "15816  [notiven, rt, nytimesworld, trump, actually, b...  \n",
       "15817  [rt, hey, liberal, climate, change, crap, hoax...  \n",
       "15818  [rt, chet, cannon, kurteichenwald, climate, ch...  \n",
       "\n",
       "[15819 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features and the label\n",
    "X = train_df['tweet tokens']\n",
    "y = train_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming the dataset\n",
    "data = train_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in data] #create your corpus here\n",
    "vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15819x19889 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 175274 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the datasets and training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scikit_log_reg = LogisticRegression(solver='liblinear',random_state=42)\n",
    "LogisticRegression_model=scikit_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = LogisticRegression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  75   29  165    9]\n",
      " [   4  121  276   24]\n",
      " [   7   36 1619   93]\n",
      " [   3   13  216  474]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.84      0.27      0.41       278\n",
      "           0       0.61      0.28      0.39       425\n",
      "           1       0.71      0.92      0.80      1755\n",
      "           2       0.79      0.67      0.73       706\n",
      "\n",
      "    accuracy                           0.72      3164\n",
      "   macro avg       0.74      0.54      0.58      3164\n",
      "weighted avg       0.73      0.72      0.70      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Logistic Regression model performance\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test.csv\n",
    "test_ = test_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in test_]\n",
    "tests_X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = LogisticRegression_model.predict(tests_X)\n",
    "predictions = pred[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10546"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10546"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df['tweetid'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating kaggle submission\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test_df['tweetid'].to_list(),\n",
    "     'sentiment': list(predictions)\n",
    "    })\n",
    "submission.to_csv(\"LogisticRegressionSubmissions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model as dump\n",
    "with open('logisticRegression.pkl', 'wb') as file:\n",
    "    pickle.dump(LogisticRegression_model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the datasets and training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = DecisionTreeClassifier()\n",
    "decisionTree_model = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = decisionTree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  80   44  134   20]\n",
      " [  23  137  230   35]\n",
      " [  61  171 1309  214]\n",
      " [  11   43  220  432]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.29      0.35       278\n",
      "           0       0.35      0.32      0.33       425\n",
      "           1       0.69      0.75      0.72      1755\n",
      "           2       0.62      0.61      0.61       706\n",
      "\n",
      "    accuracy                           0.62      3164\n",
      "   macro avg       0.53      0.49      0.50      3164\n",
      "weighted avg       0.61      0.62      0.61      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model performance\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test.csv\n",
    "test_ = test_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in test_]\n",
    "tests_X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = decisionTree_model.predict(tests_X)\n",
    "predictions_DT = pred[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating kaggle submission\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test_df['tweetid'].to_list(),\n",
    "     'sentiment': list(predictions_DT)\n",
    "    })\n",
    "submission.to_csv(\"DecisionTreeClassifierSubmission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model as dump\n",
    "with open('DecisionTreeClassifier.pkl', 'wb') as file:\n",
    "    pickle.dump(decisionTree_model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the datasets and training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "SVM_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = SVM_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 115   33  125    5]\n",
      " [   9  166  232   18]\n",
      " [  16   68 1557  114]\n",
      " [   6   21  172  507]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.41      0.54       278\n",
      "           0       0.58      0.39      0.47       425\n",
      "           1       0.75      0.89      0.81      1755\n",
      "           2       0.79      0.72      0.75       706\n",
      "\n",
      "    accuracy                           0.74      3164\n",
      "   macro avg       0.72      0.60      0.64      3164\n",
      "weighted avg       0.74      0.74      0.73      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model performance\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test.csv\n",
    "test_ = test_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in test_]\n",
    "tests_X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = SVM_model.predict(tests_X)\n",
    "predictions_SVM = pred[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating kaggle submission\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test_df['tweetid'].to_list(),\n",
    "     'sentiment': list(predictions_SVM)\n",
    "    })\n",
    "submission.to_csv(\"LinearSVM_Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test.csv\n",
    "test_ = test_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in test_]\n",
    "tests_X = vectorizer.transform(corpus)#saving model as dump\n",
    "with open('LinearSVM.pkl', 'wb') as file:\n",
    "    pickle.dump(SVM_model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the datasets and training the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = GaussianNB()\n",
    "X_train = X_train.todense()\n",
    "GaussianNB_model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "X_test = X_test.todense()\n",
    "y_pred = GaussianNB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[136  36  89  17]\n",
      " [102 120 129  74]\n",
      " [234 230 949 342]\n",
      " [ 84  29 136 457]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.24      0.49      0.33       278\n",
      "           0       0.29      0.28      0.29       425\n",
      "           1       0.73      0.54      0.62      1755\n",
      "           2       0.51      0.65      0.57       706\n",
      "\n",
      "    accuracy                           0.53      3164\n",
      "   macro avg       0.44      0.49      0.45      3164\n",
      "weighted avg       0.58      0.53      0.54      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model performance\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test.csv\n",
    "test_ = test_df['tweet tokens']\n",
    "corpus = [' '.join(i) for i in test_]\n",
    "tests_X = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_X = tests_X.todense()\n",
    "pred = GaussianNB_model.predict(tests_X)\n",
    "predictions_GaussianNB = pred[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating kaggle submission\n",
    "submission = pd.DataFrame(\n",
    "    {'tweetid': test_df['tweetid'].to_list(),\n",
    "     'sentiment': list(predictions_GaussianNB)\n",
    "    })\n",
    "submission.to_csv(\"GaussianNB_Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model as dump\n",
    "with open('GaussianNB.pkl', 'wb') as file:\n",
    "    pickle.dump(SVM_model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
